# 摄取管道

数据获取 `IngestionPipeline` 使用变换 `Transformations` 的概念。这些变换 `Transformations` 应用于我们的输入数据，并且结果节点要么返回，要么插入到矢量数据库中（如果给定）。系统缓存每个 `节点+转换对` ，以便后续使用相同 `节点+转换` 组合的运行（如果缓存被持久化）可以使用缓存结果并节省您的时间。

IngestionPipeline要查看正在使用的交互式示例，请查看RAG CLI。

## 使用模式
最简单的使用 `IngestionPipeline` 的方法是像这样实例化：

``` python
from llama_index.core import Document
from llama_index.embeddings.openai import OpenAIEmbedding
from llama_index.core.node_parser import SentenceSplitter
from llama_index.core.extractors import TitleExtractor
from llama_index.core.ingestion import IngestionPipeline, IngestionCache

# create the pipeline with transformations
pipeline = IngestionPipeline(
    transformations=[
        SentenceSplitter(chunk_size=25, chunk_overlap=0),
        TitleExtractor(),
        OpenAIEmbedding(),
    ]
)

# run the pipeline
nodes = pipeline.run(documents=[Document.example()])
```

请注意，在现实世界中，您可以SimpleDirectoryReader从 Llama Hub 或其他阅读器获取您的文档。

连接到矢量数据库#
运行摄取管道时，您还可以选择将生成的节点自动插入到远程向量存储中。

然后，您可以稍后从该向量存储构建索引。


from llama_index.core import Document
from llama_index.embeddings.openai import OpenAIEmbedding
from llama_index.core.node_parser import SentenceSplitter
from llama_index.core.extractors import TitleExtractor
from llama_index.core.ingestion import IngestionPipeline
from llama_index.vector_stores.qdrant import QdrantVectorStore

import qdrant_client

client = qdrant_client.QdrantClient(location=":memory:")
vector_store = QdrantVectorStore(client=client, collection_name="test_store")

pipeline = IngestionPipeline(
    transformations=[
        SentenceSplitter(chunk_size=25, chunk_overlap=0),
        TitleExtractor(),
        OpenAIEmbedding(),
    ],
    vector_store=vector_store,
)

# Ingest directly into a vector db
pipeline.run(documents=[Document.example()])

# Create your index
from llama_index.core import VectorStoreIndex

index = VectorStoreIndex.from_vector_store(vector_store)
计算管道中的嵌入#
请注意，在上面的示例中，嵌入是作为管道的一部分进行计算的。如果您将管道连接到向量存储，则嵌入必须是管道的一个阶段，否则您稍后对索引的实例化将失败。

如果您没有连接到向量存储，则可以从管道中省略嵌入，即仅生成节点列表。

缓存#
在 中IngestionPipeline，每个节点 + 转换组合都经过哈希处理并缓存。这可以节省使用相同数据的后续运行的时间。

以下部分介绍了一些有关缓存的基本用法。

本地缓存管理#
一旦您有了管道，您可能想要存储和加载缓存。


# save
pipeline.persist("./pipeline_storage")

# load and restore state
new_pipeline = IngestionPipeline(
    transformations=[
        SentenceSplitter(chunk_size=25, chunk_overlap=0),
        TitleExtractor(),
    ],
)
new_pipeline.load("./pipeline_storage")

# will run instantly due to the cache
nodes = pipeline.run(documents=[Document.example()])
如果缓存太大，你可以清除它


# delete all context of the cache
cache.clear()
远程缓存管理#
我们支持多个缓存远程存储后端

RedisCache
MongoDBCache
FirestoreCache
这里使用以下示例RedisCache：


from llama_index.core import Document
from llama_index.embeddings.openai import OpenAIEmbedding
from llama_index.core.node_parser import SentenceSplitter
from llama_index.core.extractors import TitleExtractor
from llama_index.core.ingestion import IngestionPipeline, IngestionCache
from llama_index.storage.kvstore.redis import RedisKVStore as RedisCache


ingest_cache = IngestionCache(
    cache=RedisCache.from_host_and_port(host="127.0.0.1", port=6379),
    collection="my_test_cache",
)

pipeline = IngestionPipeline(
    transformations=[
        SentenceSplitter(chunk_size=25, chunk_overlap=0),
        TitleExtractor(),
        OpenAIEmbedding(),
    ],
    cache=ingest_cache,
)

# Ingest directly into a vector db
nodes = pipeline.run(documents=[Document.example()])
这里不需要持久步骤，因为所有内容都会在指定的远程集合中被缓存。

异步支持#
还支持IngestionPipeline异步操作


nodes = await pipeline.arun(documents=documents)
文档管理#
附加docstore到摄取管道将启用文档管理。

使用document.doc_id或node.ref_doc_id作为接地点，摄取管道将主动寻找重复的文档。

它的工作原理是：

doc_id存储->的地图document_hash
如果附加了向量存储：
如果检测到重复doc_id，并且哈希值已更改，则将重新处理并更新文档
如果检测到重复doc_id且哈希值未改变，则跳过该节点
如果仅未附加向量存储：
检查每个节点的所有现有哈希值
如果发现重复，则跳过该节点
否则，节点被处理
注意：如果我们不附加向量存储，我们只能检查并删除重复的输入。


from llama_index.core.ingestion import IngestionPipeline
from llama_index.core.storage.docstore import SimpleDocumentStore

pipeline = IngestionPipeline(
    transformations=[...], docstore=SimpleDocumentStore()
)
完整的演练可以在我们的演示笔记本中找到。

另请查看另一个使用Redis 作为我们整个摄取堆栈的指南。

并行处理#
该run方法IngestionPipeline可以用并行进程执行。它通过将multiprocessing.Pool节点批次分布到各个处理器来实现。

要使用并行处理执行，请设置num_workers您想要使用的进程数：


from llama_index.core.ingestion import IngestionPipeline

pipeline = IngestionPipeline(
    transformations=[...],
)
pipeline.run(documents=[...], num_workers=4)
模块#
转型指南
高级摄取管道
异步摄取管道
文档管理流程
Redis 提取管道
Google Drive 提取管道
并行执行管道