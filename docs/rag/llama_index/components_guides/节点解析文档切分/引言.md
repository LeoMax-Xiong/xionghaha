## 节点解析器使用模式

节点解析器是一种简单的抽象，它获取文档列表，并将它们 **分块成Node对象**，这样每个节点都是父文档的特定块。当文档被分解成节点时，**它的所有属性都会被继承到子节点（即metadata文本和元数据模板等）**。您可以在[此处](../文件加载/文档和节点/引言.md)阅读有关Node和Document属性的更多信息。

## 入门

### 独立使用

节点解析器可以单独使用：

``` python
from llama_index.core import Document
from llama_index.core.node_parser import SentenceSplitter

node_parser = SentenceSplitter(chunk_size=1024, chunk_overlap=20)

nodes = node_parser.get_nodes_from_documents(
    [Document(text="long text")], show_progress=False
)
```

### 转换用法

节点解析器可以包含在具有摄取管道的任何一组转换中。

``` python
from llama_index.core import SimpleDirectoryReader
from llama_index.core.ingestion import IngestionPipeline
from llama_index.core.node_parser import TokenTextSplitter

documents = SimpleDirectoryReader("./data").load_data()

pipeline = IngestionPipeline(transformations=[TokenTextSplitter(), ...])

nodes = pipeline.run(documents=documents)
```

### 索引用法

在构建索引时，在内部设置 `transformations` 或者局设置可以自动使 `.from_documents()` 构建索引：

``` python
from llama_index.core import SimpleDirectoryReader, VectorStoreIndex
from llama_index.core.node_parser import SentenceSplitter

documents = SimpleDirectoryReader("./data").load_data()

# global
from llama_index.core import Settings

Settings.text_splitter = SentenceSplitter(chunk_size=1024, chunk_overlap=20)

# per-index
index = VectorStoreIndex.from_documents(
    documents,
    transformations=[SentenceSplitter(chunk_size=1024, chunk_overlap=20)],
)
``` 

## 模块

查看完整的模块指南。