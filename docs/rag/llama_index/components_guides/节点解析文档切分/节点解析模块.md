## Node 解析器模块

## 基于文件的节点解析器

有几种基于文件的节点解析器，它们是根据正在解析的内容类型（JSON、Markdown 等）创建节点。

最简单的流程是将 `SimpleFileNodeParser` 与 `FlatFileReader` 结合起来，自动为每种类型的内容使用最佳节点解析器。然后，您可能希望将 **基于文件** 的节点解析器与 **基于文本** 的节点解析器链接起来，以考虑文本的实际长度。

### 简单文件节点解析器

``` python
from llama_index.core.node_parser import SimpleFileNodeParser
from llama_index.readers.file import FlatReader
from pathlib import Path

md_docs = FlatReader().load_data(Path("./test.md"))

parser = SimpleFileNodeParser()
md_nodes = parser.get_nodes_from_documents(md_docs)
```

### HTML节点解析器

该节点解析器用于 `beautifulsoup` 解析原始 `HTML`。

默认情况下，它将解析选定的 HTML 标签子集，但您可以覆盖此功能。

默认标签为：`["p", "h1", "h2", "h3", "h4", "h5", "h6", "li", "b", "i", "u", "section"]`

``` python
from llama_index.core.node_parser import HTMLNodeParser

parser = HTMLNodeParser(tags=["p", "h1"])  # optional list of tags
nodes = parser.get_nodes_from_documents(html_docs)
```

### JSONNode解析器

`JSONNodeParser` 原始 JSON。

``` python
from llama_index.core.node_parser import JSONNodeParser

parser = JSONNodeParser()

nodes = parser.get_nodes_from_documents(json_docs)
```

### Markdown节点解析器

`MarkdownNodeParser` 解析原始 `markdown` 文本。

``` python
from llama_index.core.node_parser import MarkdownNodeParser

parser = MarkdownNodeParser()

nodes = parser.get_nodes_from_documents(markdown_docs)
```

## 文本分割器

### 代码分割器
根据编写语言来分割原始代码文本。

请在此处查看支持的语言的完整列表。

``` python
from llama_index.core.node_parser import CodeSplitter

splitter = CodeSplitter(
    language="python",
    chunk_lines=40,  # lines per chunk
    chunk_lines_overlap=15,  # lines overlap between chunks
    max_chars=1500,  # max chars per chunk
)
nodes = splitter.get_nodes_from_documents(documents)
```

### LangchainNode解析器

您还可以使用节点解析器包装 langchain 的文本分割器。

``` python
from langchain.text_splitter import RecursiveCharacterTextSplitter
from llama_index.core.node_parser import LangchainNodeParser

parser = LangchainNodeParser(RecursiveCharacterTextSplitter())
nodes = parser.get_nodes_from_documents(documents)
```

### 句子分割器

尝试 `SentenceSplitter` 在尊重句子界限的同时分割文本。

``` python
from llama_index.core.node_parser import SentenceSplitter

splitter = SentenceSplitter(
    chunk_size=1024,
    chunk_overlap=20,
)
nodes = splitter.get_nodes_from_documents(documents)
```

### 句子窗口节点解析器

与其他节点解析器类似 `SentenceWindowNodeParser`，不同之处在于它将所有文档拆分为单个句子。生成的节点还包含元数据中每个节点周围的句子“窗口”。请注意，此元数据对 LLM 或嵌入模型不可见。

这对于生成具有非常特定范围的嵌入非常有用。然后，结合 `MetadataReplacementNodePostProcessor`，您可以在将节点发送到 LLM 之前用其周围的上下文替换句子。

下面是使用默认设置设置解析器的示例。实际上，您通常只想调整句子的窗口大小。

``` python
from llama_index.core.node_parser import SentenceWindowNodeParser

node_parser = SentenceWindowNodeParser.from_defaults(
    # how many sentences on either side to capture
    window_size=3,
    # the metadata key that holds the window of surrounding sentences
    window_metadata_key="window",
    # the metadata key that holds the original sentence
    original_text_metadata_key="original_sentence",
)
```

结合 `MetadataReplacementNodePostProcessor` 可以在此处找到完整的示例。

### 语义分割节点解析器

“语义分块”是 Greg Kamradt 在他的关于 5 个嵌入分块级别的视频教程中提出的一个新概念：https://youtu.be/8OJC21T2SL4?t=1933

语义分割器不会使用固定的块大小对文本进行分块，而是使用嵌入相似性自适应地选择句子之间的断点。这确保了“块”包含语义上相互相关的句子。

我们将其改编成 LlamaIndex 模块。

查看下面我们的笔记本！

注意事项：

该正则表达式主要适用于英语句子
您可能需要调整断点百分位阈值。

``` python
from llama_index.core.node_parser import SemanticSplitterNodeParser
from llama_index.embeddings.openai import OpenAIEmbedding

embed_model = OpenAIEmbedding()
splitter = SemanticSplitterNodeParser(
    buffer_size=1, breakpoint_percentile_threshold=95, embed_model=embed_model
)
```

完整的示例可在我们的使用指南SemanticSplitterNodeParser中找到。

### TokenTextSplitter

尝试TokenTextSplitter根据原始令牌数分割成一致的块大小。

``` python
from llama_index.core.node_parser import TokenTextSplitter

splitter = TokenTextSplitter(
    chunk_size=1024,
    chunk_overlap=20,
    separator=" ",
)
nodes = splitter.get_nodes_from_documents(documents)
```

## 基于关系的节点解析器


### 层次节点解析器

此节点解析器将把节点分块为分层节点。这意味着单个输入将被分块为多个块大小的层次结构，每个节点都包含对其父节点的引用。

与 `AutoMergingRetriever` 结合使用时，这使我们能够在检索到大多数子节点时自动用父节点替换检索到的节点。此过程为 LLM 提供了更完整的响应合成上下文。

``` python
from llama_index.core.node_parser import HierarchicalNodeParser

node_parser = HierarchicalNodeParser.from_defaults(
    chunk_sizes=[2048, 512, 128]
)
```

结合 `AutoMergingRetriever` 可以在此处找到完整的示例。