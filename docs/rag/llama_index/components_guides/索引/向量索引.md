## 使用向量索引 VectorStoreIndex

向量存储是检索增强生成 (RAG) 的关键组件，因此您最终会在使用 LlamaIndex 开发的几乎每个应用程序中直接或间接地使用它们。

向量存储接受 `Node` 对象列表，并将根据这些 `Node` 对象列表构建索引

## 将数据加载到索引中

### 基本用法
使用向量存储 `Vector Store` 的最简单方法是加载一组文档并使用 `from_documents` 方法构建索引：

``` python
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader

# Load documents and build index
documents = SimpleDirectoryReader(
    "../../examples/data/paul_graham"
).load_data()
index = VectorStoreIndex.from_documents(documents)
```

!!! 注意
    如果您在命令行上使用 `from_documents`，可以方便地 `show_progress=True`在索引构建期间显示进度条。

当我们使用 `from_documents` 时，文档将被分成片段 `chunk` 并解析为 `Node`对象，这是对文本字符串的轻量级抽象，用于跟踪元数据和关系。

有关如何加载文档的更多信息，请参阅理解加载。

默认情况下，`VectorStoreIndex` 将所有内容存储在内存中。有关如何使用持久性向量存储的更多信息，请参阅下面的使用向量存储。

!!! 提示
    默认情况下，`VectorStoreIndex` 将以 **2048** 个节点为一批生成和插入向量。如果您的内存有限（或内存过剩），您可以通过传递`insert_batch_size=2048` 所需的批大小来修改此设置。
    当您插入远程托管的矢量数据库时这特别有用。

### 使用插入管道创建节点

如果您想要更好地控制文档的索引方式，我们建议您使用提取管道。这允许您自定义节点的分块、元数据和嵌入。

``` python
from llama_index.core import Document
from llama_index.embeddings.openai import OpenAIEmbedding
from llama_index.core.node_parser import SentenceSplitter
from llama_index.core.extractors import TitleExtractor
from llama_index.core.ingestion import IngestionPipeline, IngestionCache

# create the pipeline with transformations
pipeline = IngestionPipeline(
    transformations=[
        SentenceSplitter(chunk_size=25, chunk_overlap=0),
        TitleExtractor(),
        OpenAIEmbedding(),
    ]
)

# run the pipeline
nodes = pipeline.run(documents=[Document.example()])
```

!!! 提示
    您可以了解有关如何使用摄取管道的更多信息。

### 直接创建和管理节点

如果您想要完全控制索引，您可以手动创建和定义节点并将它们直接传递给索引构造函数：

``` python
from llama_index.core.schema import TextNode

node1 = TextNode(text="<text_chunk>", id_="<node_id>")
node2 = TextNode(text="<text_chunk>", id_="<node_id>")
nodes = [node1, node2]
index = VectorStoreIndex(nodes)
```

### 处理文档更新

直接管理索引时，您将需要处理随时间变化的数据源。Index类具有 **插入**，**删除**，**更新** 和 **刷新** 操作，您可以在下面了解有关它们的更多信息：

* 元数据提取
* 文档管理

## 存储向量索引
LlamaIndex 支持几十个向量存储。你可以通过传入一个来指定使用哪一个 `StorageContext`，然后在其上指定 `vector_store` 参数，如本例中使用 Pinecone 的示例：

``` python
import pinecone
from llama_index.core import (
    VectorStoreIndex,
    SimpleDirectoryReader,
    StorageContext,
)
from llama_index.vector_stores.pinecone import PineconeVectorStore

# init pinecone
pinecone.init(api_key="<api_key>", environment="<environment>")
pinecone.create_index(
    "quickstart", dimension=1536, metric="euclidean", pod_type="p1"
)

# construct vector store and customize storage context
storage_context = StorageContext.from_defaults(
    vector_store=PineconeVectorStore(pinecone.Index("quickstart"))
)

# Load documents and build index
documents = SimpleDirectoryReader(
    "../../examples/data/paul_graham"
).load_data()
index = VectorStoreIndex.from_documents(
    documents, storage_context=storage_context
)

```

有关如何使用 `VectorStoreIndex` 的更多示例，请参阅我们的向量存储索引使用示例笔记本。

有关如何将 `VectorStoreIndex` 与特定向量存储一起使用的示例，请查看存储部分下的向量存储。

## 组合检索召回

`VectorStoreIndex` (以及任何其他索引/检索器)能够检索通用对象，包括

* 节点引用
* 查询引擎
* 召回器
* 查询管道

如果这些对象是可以被检索到，它们将使用提供的查询自动运行。

例如：

``` python
from llama_index.core.schema import IndexNode

query_engine = other_index.as_query_engine
obj = IndexNode(
    text="A query engine describing X, Y, and Z.",
    obj=query_engine,
    index_id="my_query_engine",
)

index = VectorStoreIndex(nodes=nodes, objects=[obj])
retriever = index.as_retriever(verbose=True)
```

如果检索到包含查询引擎的索引节点，则将运行查询引擎并将生成的响应作为节点返回。

欲了解更多详情，请查看指南