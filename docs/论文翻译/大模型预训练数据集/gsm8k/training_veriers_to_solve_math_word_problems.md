# Training Veri ers to Solve Math Word Problems

## 摘要

最先进的语言模型可以在许多任务上与人类的表现相匹配，但它们仍然难以稳健地执行多步数学推理。为了找出当前模型的问题并支持研究，我们引入了GSM8K，这是一个包含8.5K个高质量多语言的小学数学问题的数据集。我们发现，尽管这个问题分布在概念上很简单，但即使是最大的变压器模型也无法实现高测试性能。为了提高性能，我们建议训练验证器来判断模型完成的正确性。在测试时，我们生成许多候选解决方案，并选择验证器排名最高的一个。我们证明，验证显著提高了GSM8K的性能，我们提供了强有力的经验证据，表明验证在数据增加的情况下比微调基线更有效地扩展。

## 1 介绍

近年来，大型语言模型在许多不同的任务中表现出了令人印象深刻的技能（Wang等人，2019；Brown等人，2020）。Kaplan等人（2020）描述了增加模型大小的一致好处，描述了跨越许多数量级的缩放趋势。然而，即使是最大的模型在需要执行多步数学推理时也会动摇（Hendrycks等人，2021）。即使在模型经过适当的微调后，模型样本也经常包含灾难性的错误。因此，数学推理揭示了现代语言模型的一个关键弱点。
数学推理中的一个重大挑战是对个体错误的高度敏感性（Shen等人，2021a）。在生成解时，递归模型没有纠正自身错误的机制。迅速偏离轨道的解决方案变得无法恢复。如果我们纯粹依赖生成¬方法并从当前趋势中推断，我们将需要一个过高的
图1:GSM8K的三个示例问题。计算注释以红色突出显示。

参数计数，以在与MATH数据集一样具有挑战性的分布上实现中等性能（Hendrycks等人，2021）。这一证据有力地推动了寻找具有更有利标度律的方法。
我们建议使用训练验证器来评估模型生成解决方案的正确性，类似于Shen等人（2021a）的并发工作。在测试时，我们对固定数量的候选解决方案进行采样，并选择验证器排名最高的解决方案。验证器既受益于其固有的可选性，也受益于验证是一项比一般生成更简单的任务。

为了促进研究，我们发布了GSM8K，这是一个包含8.5K高质量小学数学问题数据集。我们设计了这个数据集，使其具有极大的语言多样性，同时依赖于相对简单的小学数学概念。最好的语言模型很难在这个数据集上具有极好的性能，主要是由于问题具有极大的多样性。同时，GSM8K解决方案仅依赖于基本概念，因此实现高测试性能是一个易于实现的目标。
我们的主要贡献如下：

1. 我们展示了一个经过精心策划的8.5K小学数学问题和自然语言解决方案的数据集，可用于探索大型语言模型的非正式推理能力。
2. 我们发现，与微调基线相比，使用验证器可以获得与模型大小增加30倍大致相同的性能提升，并且验证器的效果随着数据的增加而提升得更好。
3. 我们证明dropout是一个强正则化器，显著提高了微调和验证的性能。

## 2 数据集

GSM8K由人类问题编写者创建的8.5K个高质量的小学数学问题组成。我们将这些问题分为7.5K训练问题和1K测试问题。这些问题需要2到8个步骤来解决，解决方案主要涉及使用基本算术运算（+−×÷）执行一系列基本计算，以获得最终答案。一个聪明的中学生应该能够解决每一个问题。

我们基于以下设计原则创建了GSM8K。

* 高质量我们避免了容易出错的刮擦程序，而是依靠人工制造问题。在根据工人的回答协议进行广泛的质量控制后，我们估计不到2%的问题包含断裂错误。
* 高多样性我们努力在问题之间实现高多样性。我们积极避免设计来自同一语言模板或仅在表面细节上有所不同的问题，这是许多其他数据集中普遍存在的问题。通过使每个问题相对独特，保持测试性能成为一个更相关的指标。
* 中等难度我们选择一个对大型最先进的语言模型具有挑战性的问题分布，而不是完全棘手的。GSM8K将帮助我们更好地了解不同模型和方法在这一困难点上的数据缩放趋势。问题不需要超越早期代数水平的概念，绝大多数问题都可以在不显式定义变量的情况下解决。
* 自然语言解决方案我们收集自然语言的解决方案，而不是纯粹的数学表达式。我们认为这是最常用的数据格式，我们希望它能揭示大型语言模型内部独白的特性。我们指示问题编写者尽可能多地解释他们的工作，但我们允许他们用自己不同的语言风格编写解决方案。

完整的GSM8K数据集可以在https://github.com/openai/grade-school-math。示例问题如图1所示，我们在附录A中讨论了其他数据集细节。

![第一张图片](./img/1.png)

## 3 相关工作

### 3.1 相关数据集

早期的数学问题数据集（Kushman等人，2014；Roy和Roth，2015）相对较小，不适合测试现代语言模型的局限性。Dolphin18K（Huang等人，2016）是一个更大的数据集，包含18K问题，但解决方案仅以方程或最终答案的形式提供。AQuA RAT（Ling等人，2017）包含10万个问题，但不幸的是，该数据集存在高度的问题模板化和自然语言解决方案质量控制不佳的问题。MathQA是AQuA RAT最近发布的一个子集，专注于纠正这些错误（Amini等人，2019），但即使是纠正后的数据集也存在数据质量问题，约30%的数据存在不一致（Miao等人，2021）。Ape210K（赵等人，2020）是最大的公开数据集，由210K个中国小学数学问题组成。然而，由于语言障碍和缺乏自然语言解决方案，我们无法在此数据集上评估我们的方法。

最近开发的ASDiv数据集（Miao等人，2021）包含2.3K个数学单词问题，通过确保问题具有高多样性和高质量，解决了先前数据集中的常见缺陷。我们在GSM8K的创建过程中分享了这些设计原则。然而，我们注意到GSM8K更大，提供自然语言解决方案，并且平均包含需要更多步骤来解决的问题。MATH数据集（Hendrycks等人，2021）比GSM8K更大、更复杂，但鉴于目前最先进的语言模型的能力，其高难度使得准确衡量进展变得具有挑战性。

其他最近与推理相关的数据集侧重于符号数学的数学推理（Lample和Charton，2019）、阅读理解（LogiQA）（Liu等人，2020）和常识问答（Common senseQA）（Talmor等人，2018）。与CommonsenseQA类似，GSM8K包括需要基本背景知识的问题，如一周的天数。与要求阅读理解和逻辑推理相结合的LogiQA类似，**GSM8K的主要困难在于正确解释问题和通过解决问题的步骤进行推理。**

### 3.2 相关方法

之前的工作试图用基于循环神经网络的seq2seq模型（Sutskever等人，2014）和密切相关的变体（Wang等人，2017；Huang等人，2018）来解决经典的数学问题基准。最近的工作通过设计专门的编码器-解码器架构提高了性能（Amini等人，2019；Chiang和Chen，2018；Xie和Sun，2019；Chen等人，2020；Li等人，2020），其中最强的结果通常依赖于BERT家族的大型预训练编码器（Chen等人，2019年；Kim等人，2020年；Liang等人，2021年）。

最近的其他工作建议进行额外的预训练任务，以进一步提高基于大型transformer模型的数学推理能力。Hendrycks等人（2021）在新的AMPS语料库上提出了预训练模型，该语料库来源于Khan Academy问题和Mathematica脚本。同样，Shen等人（2021b）提出了一种从互联网上提取的学前至大学课程语料库的预训练方法，Peng等人（2021）提出了通过预测表达式树中的掩码子表达式进行预训练的方法。

与验证类似，其他方法也对语言模型进行了微调，以在众多模型完成中进行选择。Nichols等人（2020）提出了一种样本和排名方法，以提高大型语言模型的协作讲故事能力，训练数据来自人类工作者的偏好。在与我们自己密切相关的并行工作中，Shen等人（2021a）应用了类似的方法来解决数学单词问题，联合训练一个模型来生成和排序解。我们的工作与他们的方法有许多基本相似之处，尽管我们在几个关键方面有所不同。首先，我们关注自然语言解的空间，因为这是一种比纯数学表达式更丰富、更通用的解格式。此外，这一选择使我们的模型能够发展语言分析技能，并产生更容易被人类解释的解决方案。其次，我们提供的证据表明，验证者使用额外数据比基线方法更有利。最后，我们使用单独的生成器和验证器网络，以防止生成器过拟合。


## 4 方法

我们研究了解决GSM8K中问题的两种方法：微调和验证。微调是我们的基线方法，它使用与GPT-3中的生成性预训练相同的语言建模目标（Brown等人，2020）。在测试时，我们通过自回归采样单个低温解并检查最终答案是否正确来判断性能。相比之下，验证包括对多个高温解决方案进行采样，为每个解决方案分配一个分数，并输出排名最高的解决方案。验证者被训练来判断解的正确性，训练数据仅取决于解是否达到正确的最终答案。

对于这两种方法，我们使用GPT-3系列的模型作为初始化，主要关注175B和6B的模型尺寸。175B模型是最大的，并产生了最令人印象深刻的结果，而6B模型在研究目的上明显更方便。我们在附录B中讨论了超参数的选择。
我们的模型经常不能准确地进行计算。尽管较大的模型比较小的模型犯的算术错误更少，但这仍然是常见的错误来源。为了缓解这个问题，我们通过向训练集中注入计算注释来训练所有模型使用计算器。在测试时，当模型选择使用这些注释时，计算器将覆盖采样。详情见附录C。

### 4.1 微调

我们通过更新模型参数来执行微调，以最小化所有训练token的交叉熵损失。图2显示了在20个迭代周期内对不同大小的训练集进行微调后的测试性能。我们将相同的数据可视化为训练集大小的函数和模型大小的函数。测试性能由每个测试问题的单个低温（T=0）样本决定。不出所料，我们看到175B型号的表现明显优于较小的型号。假设呈对数线性趋势，我们可以天真地推断这些结果，以估计在使用完整的GSM8K训练集时，需要具有1016个参数的模型才能达到80%的求解率。更难沿数据维度进行推断，因为性能似乎并不遵循对数线性趋势。然而，175B模型似乎需要至少两个数量级的额外训练数据才能达到80%的解决率。
![第二张图](./img/2.png)

在图3中，我们展示了6B测试性能在100训练时期。我们使用test@N当允许模型对每个问题进行N次单独猜测时，表示至少一次正确解决的问题的百分比。我们使用低温（T=0）来产生test@1我们使用更高的温度（T=0.7）来生成test@100样本。这两个温度值都是根据经验选择的，以产生最佳结果。Test@1性能几乎单调地提高，即使我们很快开始对测试损失进行过拟合。不幸的是，test@100性能下降的幅度比test@1随着我们增加纪元的数量。这是意料之中的：随着模型反复遇到相同的数据，它对预测变得越来越不校准和过于自信。在测试时，这种过度自信会导致解决方案空间的覆盖率低，这种影响只有在我们在测试时考虑多个样本时才会变得明显。
选择一个具有良好覆盖率的模型对于成功训练验证者至关重要。根据经验，我们看到test@100性能在最初的几个时期达到峰值。因此，我们使用经过2个epoch训练的模型来生成训练验证器的样本。我们在附录D中提供了6B和175B模型的几个示例解决方案。我们还注意到，在输出最终答案之前，让模型生成完整的自然语言解决方案非常重要。如果我们改为微调6B模型，直接输出最终答案，而不进行任何中间步骤，性能将从20.6%急剧下降到5.2%。

![第三张图](./img/3.png)

### 4.2 验证

为了改进微调基线，我们训练验证器来判断模型生成解决方案的正确性，并在测试时根据这些验证器进行搜索。在问题和候选解决方案的条件下，验证器输出解决方案正确的概率。训练解决方案仅根据其是否达到正确的最终答案来标记为正确或不正确。在实践中，一些解决方案会使用有缺陷的推理得出正确的最终答案，从而导致误报。
![第四张图](./img/4.png)
如图4所示，我们按如下方式训练验证器：

1. 在训练集上对模型（“生成器”）进行2个迭代周期的微调。
2. 从生成器中为每个训练问题抽取100个完成样本，并将每个解决方案标记为正确或不正确。
3. 在此数据集上训练单个历元的验证器。

训练2个epoch就足以让生成器学习该领域的基本技能。我们选择不进行更长时间的训练，因为在此之后生成的解决方案的多样性开始崩溃，如图3所示。我们训练单独的生成器和验证器模型来限制生成器的训练并防止过拟合，但原则上，应该可以组合这些模型。除非另有说明，否则我们对生成器和验证器使用相同的模型大小。除了预测解决方案的正确性外，我们还使用与生成器相同的语言建模目标来训练验证器。这对验证者来说是一个有价值的辅助目标。我们在附录E中讨论了其他验证者培训细节。

在测试时，我们对每个测试问题的100个完成情况进行抽样，用验证器对其进行排名，然后返回验证器得分最高的一个。图5显示了6B和175B模型尺寸的验证和微调之间的比较。我们发现，在低数据集大小下使用验证是不利的。我们认为这是由于对正确答案进行过拟合的压力造成的：对于小数据集，对正确答案的过拟合比学习正确推理的更具普遍性的属性更快。然而，一旦我们使用了足够大的数据集，我们就会看到验证器的强大推动力。
![第五张图](./img/5.png)

值得注意的是，175B验证器比6B验证器“起飞”得早，需要更少的训练问题才能超过微调基线。验证者找到的示例解决方案见附录D，验证者置信度可视化见附录F。

### 4.3 验证消融实验

我们可以训练验证器，以整个生成的解为条件进行单个标量预测，也可以在解中的每个令牌之后进行标量预测。默认情况下，我们选择后者，训练验证器在每个令牌后进行预测。这可以被视为一个令牌级别的值函数。我们在图6a中比较了这两种方法，分别标记为“解决方案级别”和“令牌级别”。

预测每个令牌的值函数是一项比仅判断完全完成更具挑战性和更嘈杂的任务。然而，尽管最初的训练速度较慢，但令牌级验证器最终仍优于解决方案级验证器。此外，令牌级验证器在训练后期仍在改进，而解决方案级验证器则很快显示出过拟合的迹象。我们假设全值函数提供了一个有用的辅助信号，鼓励模型在整个解决方案中判断推理，而不仅仅是记忆正确的最终答案。
在图6b中，我们消除了训练验证者时使用的目标。如第4.2节所述，我们可以选择在验证目标旁边包含语言建模目标。我们将使用这两个目标与仅使用验证目标进行比较。虽然两者都是合理的选择，但包括语言建模目标在内的严格改进。这使得直观意义：更好地理解这种语言分布只会帮助验证者区分样本。
![第六张图](./img/6.png)

在图6c中，我们分别消融生成器和验证器的模型大小。我们发现，使用大型生成器和小型验证器的性能明显优于使用小型生成器和大型验证器的性能。即使验证器比生成器小得多，验证仍然非常有效。这表明验证者可能经常依赖相对粗略的启发式方法来区分来自给定生成器的解决方案，而不是尝试更彻底的验证形式。

## 5 附加实验

### 5.1 测试时间计算

在测试时，我们可以选择生成任意多个解决方案，由验证者在选择排名最高的完成之前进行判断。图7a显示了6B验证器的性能如何随每个测试问题的完成次数而变化。在这种规模下，随着我们将完工数量增加到400个，性能也会提高。超过这一点，性能开始下降。这表明，搜索的好处最终被找到欺骗验证者的对抗性解决方案的风险所抵消。一般来说，我们使用100个补全来评估验证器测试性能，因为这以相对适中的计算成本获得了验证的大部分好处。
![第七张图](./img/7.png)
为了进一步提高性能，我们可以在排名靠前的验证者解决方案中进行多数投票，而不是只选择一个顶级解决方案。
此投票过程只考虑各个解决方案得出的最终答案：最终选择的答案是得票最多的答案。图7b显示了当我们允许更多的顶级样本投票时，性能是如何变化的。不出所料，当从更多的样本开始时，我们可以允许更多的样本进行投票。当我们只有100个样本时，最好只允许前3-5个样本投票。当我们有3200个样本时，让前30名投票大约是最佳选择。

### 5.2 规范化

我们发现，使用dropout作为正则化器对微调和验证都有很大的好处。具体来说，我们沿着网络中每一层的残差路径应用残差dropout（Vaswani等人，2017）。我们根据超参数扫描的结果选择20%的dropout用于所有dropout实验。我们注意到GPT-3模型没有预先训练dropout。因此，对于涉及dropout的实验，我们在随后微调模型之前，使用dropout进行额外的预训练。这减轻了模型在微调过程中经历的分布变化。

我们首先研究了 dropout 对不同训练集大小的微调的影响。图8a显示，dropout 导致比基线有显著改善。接下来，我们将研究dropout对验证器的影响，同时考虑解决方案级别和令牌级别的变量。在图8b中，我们看到dropout显著提高了解决方案级验证器，减轻了非正则化基线中发生的过拟合。值得注意的是，使用dropout和解决方案级验证器可以达到与令牌级验证器类似的性能水平。在图8c中，我们将dropout应用于令牌级验证器。由于令牌级验证器已经不太容易过拟合，因此dropout的影响较小也就不足为奇了。尽管如此，我们仍然看到，通过使用dropout训练令牌级验证器略有收获。请注意，我们将令牌级验证器的批大小增加了4倍，以更好地处理更困难的目标和辍学带来的噪音。
![第八张图](./img/8.png)

## 6 结论

我们已经看到，相对于微调基线，验证提供了显著的性能提升。在完整数据集上，6B验证略优于微调的175B模型，从而提供了大约相当于模型大小增加30倍的提升。我们还看到，令牌级验证器比解决方案级验证器更不容易过拟合，并且所有方法都受益于带有残差丢弃的正则化。我们希望验证能够很好地扩展到需要更复杂数学推理的问题分布，我们希望GSM8K支持开发扩展性更好的新方法。


## 数据集详细信息

我们最初通过在Upwork（¬Upwork.com）上雇佣自由承包商收集了1000个问题和自然语言解决方案。然后，我们与自然语言处理数据标签平台Surge AI（surgehq.AI）合作，扩大了我们的数据收集规模。在收集了完整的数据集后，我们要求工人重新解决所有问题，没有工人重新解决他们最初写的问题。我们检查了他们的最终答案是否与最初的解决方案一致，任何产生分歧的问题要么被修复，要么被丢弃。然后，我们对一小部分问题进行了另一轮协议检查，发现1.7%的问题仍然会在承包商之间产生分歧。我们估计这是包含断裂错误或歧义的问题的一部分。可能有更大比例的问题包含微妙的错误。
为了帮助承包商编写问题，我们提供了从几个镜头提示的175B GPT-3模型自动生成的种子问题。承包商可以直接使用这些种子问题，将其作为灵感并进行修改，或者完全提出自己的问题。我们指示承包商在解决方案中尽可能具有描述性，并且不要在不同问题之间重复使用问题设置或模板。为了确保承包商不会重复使用问题模板，我们计算了问题之间的成对相似性得分，并以此向承包商提供反馈。


# 超参数
我们在下面列出了一个重要的超参数表。我们根据表中的值在两个方向上对学习率和批量大小进行了一个数量级的扫描，但没有发现任何显著的改进。验证器温度（例如：1.0而不是0.7）和目标（交叉熵而不是均方误差）的其他合理选择对我们的消融影响也可以忽略不计。
![第9张图](./img/9.png)


## 计算器注释

计算器注释不是由人工承包商提供的：它们是由硬编码逻辑和微调的语言模型组合生成的。自动生成计算器注释的逻辑并不完美。它不太可能生成任何不正确的注释，但忽略一些可以注释的行并不罕见。


在训练过程中，带注释的令牌和解决方案的其余部分之间没有特殊的区别：它们都只是令牌。在测试过程中，当存在格式良好的注释时，我们会覆盖模型采样，特别是覆盖直接在“=”后面和¬<<…>>内的标记。
为了模拟计算器，我们只需使用python eval函数来计算表达式中的标记（图9）。超时或抛出错误的评估会导致跳过注释，并像往常一样对模型进行采样。¬

我们注意到，用于本文所有结果的计算器的原始版本存在一些小的实现错误。因此，我们报告的测试性能略有低估，尽管在大多数实验中，这种差异的幅度不到1%。使用完整的GSM8K训练集时，修复计算器可将验证测试性能提高约1%。
![第十张图](./img/10.png)

## 示例模型解决方案

我们展示了一些样本，比较了6B和175B尺度下的微调和验证。为了多样性，对样本进行了轻微的挑选。
![第十一张图](./img/11.png)
![第十二张图](./img/12.png)

## 验证器详细信息

如第4.2节所述，除了原始语言建模目标外，我们还训练具有联合目标的验证器，其中模型学会将模型完成标记为正确或不正确。从架构上讲，这意味着我们的验证器是语言模型，具有一个小的标量头，可以在每个令牌的基础上输出预测。

我们将这个标量头实现为单个偏置参数和单个增益参数，它们对语言模型最终未嵌入层输出的logits进行操作。具体来说，偏置和增益会移动并缩放与词汇表中的特殊标记相对应的logit。因此，其他令牌的logits可以继续表示语言建模目标，而这个特殊的令牌是为验证器的预测保留的。

我们可以选择从生成器微调的同一预训练语言模型或生成器本身初始化验证器。在我们的消融中，后者的表现略好；我们怀疑这是因为更好地理解生成器学习的语言分布只会帮助验证器对该分布中的样本进行评分。除非另有明确说明，否则在所有实验中，我们都会从相应的生成器初始化验证器。

当训练具有共同目标的验证者时，我们使用语言数据和验证者数据的相等组合。因为我们对每个原始训练示例的100个完成进行采样以生成验证器数据，所以使用相等的混合意味着我们有效地将原始语言数据上采样了100倍。为了形成联合目标，我们只需添加验证者损失和未加权的语言建模损失，并将该联合目标的一个纪元定义为每个验证者示例见过一次。有了这两个目标，我们屏蔽了问题中的令牌，只在解决方案中的令牌上进行训练，如图12所示。
![第十三张图](./img/13.png)

## 验证器可视化

![第十四张图](./img/14.png)
令牌级验证器的一个好处是，这些模型可以立即解释：我们可以可视化每个令牌的预测值，更好地理解验证器如何判断样本。上面我们展示了五个不同精心挑选的问题和模型完成的预测值的可视化，由在完整训练集上训练的175B令牌级别验证器验证。
在可视化中，文本的背景颜色对应于该令牌的验证器分数，其中红色是低值（预测不正确），绿色是低值是高值（预测正确）。表格的第二列总结了验证者的预测，第三列表明生成的模型完成实际上是正确的还是不正确的。第二列和第三列之间的任何不一致都表明验证者犯了错误。
第一行包括一个真正的正示例，其中验证器正确地将完成分类为正确。请注意，模型最初不确定解是否正确，并随着解的进展逐渐获得确定性：这可能是验证器训练过程的一个特性，它在很大一部分不正确的模型生成样本上进行训练。
第二行包含一个问题，其中解决方案是正确的，但验证者将其评为不正确。这可能是由于问题描述中的“4次”和“4个土豆”之间存在歧义。
第三行包含另一个假阴性示例。然而，与前面的例子不同，这里的模型完成包含一些错误的推理。因此，尽管模型完成中的最终答案是正确的，但自然语言解释是不正确的，因此验证者正确地给出了较低的分数。
在第四行中，我们看到验证者对模型完成度进行评分，该模型完成度一开始是正确的，但随着解决方案的进展，验证者逐渐对解决方案失去信心。在解决方案出现明显错误（说花费了64美元，而不是64+16+8=88美元）后，验证者以高度的置信度判断解决方案不正确。
最后一行包含一个误报，即模型在第二步出错，即从钻石珠宝而不是黄金珠宝的价格中减去400。验证者偶尔会在将数量与其关系进行变量绑定时出错。
