# 安装

您可以使用以下任一方法安装 SGLang。

## 方法 1：使用 pip 安装

``` shell
pip install --upgrade pip
pip install "sglang[all]" --find-links https://flashinfer.ai/whl/cu121/torch2.4/flashinfer/
``` 

注意：请检查 `FlashInfer` 安装文档，根据您的 PyTorch 和 CUDA 版本安装正确的版本。

## 方法 2：从源
# Use the last release branch
git clone -b v0.4.0.post1 https://github.com/sgl-project/sglang.git
cd sglang

pip install --upgrade pip
pip install -e "python[all]" --find-links https://flashinfer.ai/whl/cu121/torch2.4/flashinfer/
注意：请检查FlashInfer 安装文档，根据您的 PyTorch 和 CUDA 版本安装正确的版本。

注意：对于带有 Instinct/MI GPU 的 AMD ROCm 系统，请执行以下操作：

# Use the last release branch
git clone -b v0.4.0.post1 https://github.com/sgl-project/sglang.git
cd sglang

pip install --upgrade pip
pip install -e "python[all_hip]"
方法 3：使用
Docker 镜像可在 Docker Hub 上作为lmsysorg/sglang获得，由Dockerfile构建。将<secret>以下内容替换为您的 huggingface hub令牌。

docker run --gpus all \
    --shm-size 32g \
    -p 30000:30000 \
    -v ~/.cache/huggingface:/root/.cache/huggingface \
    --env "HF_TOKEN=<secret>" \
    --ipc=host \
    lmsysorg/sglang:latest \
    python3 -m sglang.launch_server --model-path meta-llama/Llama-3.1-8B-Instruct --host 0.0.0.0 --port 30000
注意：对于带有 Instinct/MI GPU 的 AMD ROCm 系统，建议使用它docker/Dockerfile.rocm来构建映像，示例和用法如下：

docker build --build-arg SGL_BRANCH=v0.4.0.post1 -t v0.4.0.post1-rocm620 -f Dockerfile.rocm .

alias drun='docker run -it --rm --network=host --device=/dev/kfd --device=/dev/dri --ipc=host \
    --shm-size 16G --group-add video --cap-add=SYS_PTRACE --security-opt seccomp=unconfined \
    -v $HOME/dockerx:/dockerx -v /data:/data'

drun -p 30000:30000 \
    -v ~/.cache/huggingface:/root/.cache/huggingface \
    --env "HF_TOKEN=<secret>" \
    v0.4.0.post1-rocm620 \
    python3 -m sglang.launch_server --model-path meta-llama/Llama-3.1-8B-Instruct --host 0.0.0.0 --port 30000

# Till flashinfer backend available, --attention-backend triton --sampling-backend pytorch are set by default
drun v0.4.0.post1-rocm620 python3 -m sglang.bench_one_batch --batch-size 32 --input 1024 --output 128 --model amd/Meta-Llama-3.1-8B-Instruct-FP8-KV --tp 8 --quantization fp8
方法 4：使用 docker 
更多的
在 Kubernetes 或云上运行
更多的
常见注释
FlashInfer是默认的注意内核后端。它仅支持 sm75 及以上版本。如果您在 sm75+ 设备（例如 T4、A10、A100、L4、L40S、H100）上遇到任何与 FlashInfer 相关的问题，请通过在GitHub 上添加并打开问题来切换到其他内核。--attention-backend triton --sampling-backend pytorch

如果您只需要将 OpenAI 模型与前端语言一起使用，则可以避免使用来安装其他依赖项。pip install "sglang[openai]"

语言前端独立于后端运行时运行。您可以在本地安装前端而无需 GPU，而后端则可以在支持 GPU 的机器上设置。要安装前端，请运行，对于后端，请使用。这允许您在本地构建 SGLang 程序并通过连接到远程后端来执行它们。pip install sglangpip install sglang[srt]