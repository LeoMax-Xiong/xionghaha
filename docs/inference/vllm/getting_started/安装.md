## 安装

vLLM 是一个 Python 库，还包含预编译的 C++ 和 CUDA（12.1）二进制文件。

##  要求

* 操作系统：Linux
* Python：3.9 - 3.12
* GPU：计算能力7.0或更高（例如V100、T4、RTX20xx、A100、L4、H100等）

## 安装已发布的版本

我们可以使用 `pip` 安装已经发布的 vLLM  版本：

``` python
# (Recommended) Create a new conda environment.
conda create -n myenv python=3.10 -y
conda activate myenv

# Install vLLM with CUDA 12.1.
pip install vllm
```

!!! 注意
    虽然我们建议使用conda来创建和管理 Python 环境，但强烈建议使用pip来安装 vLLM。这是因为 pip 安装的 torch 可以与其他的库分开安装，例如 NCCL。而 conda 则使用静态链接的方式进行安装 NCCL。当 vLLM 尝试使用 NCCL 时，这可能会导致问题。有关更多详细信息，请参阅此问题。

!!! 注意
    截至目前，vLLM 的二进制文件默认使用 CUDA 12.1 和PyTorch 公开发行版本进行编译。我们还提供使用 CUDA 11.8 和 PyTorch 公共发行版本编译的 vLLM 二进制文件：
    ``` shell
    # Install vLLM with CUDA 11.8.
    export VLLM_VERSION=0.6.1.post1
    export PYTHON_VERSION=310
    pip install https://github.com/vllm-project/vllm/releases/download/v${VLLM_VERSION}/vllm-${VLLM_VERSION}+cu118-cp${PYTHON_VERSION}-cp${PYTHON_VERSION}-manylinux1_x86_64.whl --extra-index-url https://download.pytorch.org/whl/cu118
    ```
    为了提高性能，vLLM 必须编译许多 cuda 内核。不幸的是，编译会导致与其他 CUDA 版本, PyTorch 版本的产生二进制不兼容的问题，即使对于具有不同构建配置的同一个 PyTorch 版本也是如此。
    因此，建议使用全新的 conda 环境安装 vLLM。如果您拥有不同的 CUDA 版本或想要使用现有的 PyTorch 安装，则需要从源代码构建 vLLM。请参阅后续的下文了解说明。

## 安装最新的版本

LLM 推理是一个快速发展的领域，最新的代码可能包含问题修复、性能改进和尚未发布的新功能。为了让用户无需等待下一个版本即可尝试最新代码，vLLM 为自 v0.5.3 以来的每次提交都提供了在 x86 平台上运行且带有 CUDA 12 的 Linux 部署包。您可以使用以下命令下载并安装它：

``` shell
pip install https://vllm-wheels.s3.us-west-2.amazonaws.com/nightly/vllm-1.0.0.dev-cp38-abi3-manylinux1_x86_64.whl
```

如果您想要访问以前的提交，您可以在 URL 中指定提交哈希：

``` shell
export VLLM_COMMIT=33f460b17a54acb3b6cc0b03f4a17876cff5eafd # use full commit hash from the main branch
pip install https://vllm-wheels.s3.us-west-2.amazonaws.com/${VLLM_COMMIT}/vllm-1.0.0.dev-cp38-abi3-manylinux1_x86_64.whl
``` 
请注意，这些部署包是使用 Python 3.8 ABI 构建的（有关 ABI 的更多详细信息，请参阅PEP 425 ），因此它们与 Python 3.8 及更高版本兼容。轮子文件名 (1.0.0.dev) 中的版本字符串只是一个占位符，用于为部署包提供统一的 URL。部署包的实际版本包含在部署包的元数据中。虽然我们不再支持 Python 3.8（因为 PyTorch 2.5 放弃了对 Python 3.8 的支持），但部署包仍然是使用 Python 3.8 ABI 构建的，以保持与以前相同的轮子名称。

访问最新代码的另一种方法是使用 docker 镜像：

``` python
export VLLM_COMMIT=33f460b17a54acb3b6cc0b03f4a17876cff5eafd # use full commit hash from the main branch
docker pull public.ecr.aws/q9t5s3a7/vllm-ci-test-repo:${VLLM_COMMIT}
```

这些 docker 镜像仅用于 CI 和测试，不适用于生产用途。它们将在几天后过期。

最新代码可能存在bug，且不稳定，请谨慎使用。

## 源码编译
### 仅 Python 构建(不包含编译)
如果您只需要更改 Python 代码，则只需构建 vLLM 而无需编译。

第一步是安装最新的 vLLM 部署包：
``` shell
pip install https://vllm-wheels.s3.us-west-2.amazonaws.com/nightly/vllm-1.0.0.dev-cp38-abi3-manylinux1_x86_64.whl
```
您可以在上面找到有关 vLLM 轮子的更多信息。

验证安装成功后，可以使用以下脚本：
``` shell
git clone https://github.com/vllm-project/vllm.git
cd vllm
python python_only_dev.py
```
该脚本将：

* 在当前环境中找到已安装的vLLM包。
* 将构建的文件复制到当前目录。
* 重命名已安装的 vLLM 包。
* 将当前目录符号链接到已安装的 vLLM 包。

现在，您可以在当前目录中编辑 Python 代码，并且运行 vLLM 时将反映更改。

一旦您完成编辑或者想要安装另一个 vLLM 轮子，您应该使用带有（或简称）标志的相同脚本退出开发环境：`--quit-dev-q`

``` shell
python python_only_dev.py --quit-dev
```

该 `--quit-dev` 标签将：

* 从当前目录删除到 vLLM 包的符号链接。
* 从备份中恢复原始 vLLM 包。

如果您更新 vLLM 轮子并从源头重建以进行进一步的编辑，则需要再次重复仅限 Python 的构建步骤。

!!! 笔记
    您的源代码可能与最新的 vLLM wheel 具有不同的提交 ID，这可能会导致未知错误。建议对源代码使用与您安装的 vLLM wheel 相同的提交 ID。有关如何安装指定 wheel 的说明，请参阅上一节。

### 完整构建（带编译）
如果要修改 C++ 或 CUDA 代码，则需要从源代码构建 vLLM。这可能需要几分钟：
``` shell
git clone https://github.com/vllm-project/vllm.git
cd vllm
pip install -e .
```
!!! 提示
    从源代码构建需要进行大量编译。如果您要反复从源代码构建，缓存编译结果会更有效率。例如，您可以使用或安装ccache。只要命令可以找到二进制文件，构建系统就会自动使用它。第一次构建后，后续构建将快得多。conda install ccacheapt install ccachewhich ccacheccache

#### 使用现有的 Python 安装
在某些情况下，PyTorch 依赖项无法轻松通过 pip 安装，例如：

* 使用 PyTorch 每晚或自定义 PyTorch 构建 vLLM。

* 使用 aarch64 和 CUDA (GH200) 构建 vLLM，其中 PyTorch 轮子在 PyPI 上不可用。目前，只有 PyTorch nightly 有带 CUDA 的 aarch64 轮子。您可以运行安装PyTorch nightly，然后在其基础上构建 vLLM。`pip3 install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu124`

要使用现有的 PyTorch 安装构建 vLLM：
``` shell
git clone https://github.com/vllm-project/vllm.git
cd vllm
python use_existing_torch.py
pip install -r requirements-build.txt
pip install -e . --no-build-isolation
```

#### 检查
为了避免系统过载，你可以通过环境变量来限制同时运行的编译作业的数量MAX_JOBS。例如：

``` shell
export MAX_JOBS=6
pip install -e .
```

这在性能较弱的机器上构建时尤其有用。例如，当您使用 WSL 时，它默认仅分配总内存的 50%，因此使用可以避免同时编译多个文件并耗尽内存。副作用是构建过程会慢得多。export MAX_JOBS=1

此外，如果您在构建 vLLM 时遇到困难，我们建议您使用 NVIDIA PyTorch Docker 映像。

``` shell
# Use --ipc=host to make sure the shared memory is large enough.
docker run --gpus all -it --rm --ipc=host nvcr.io/nvidia/pytorch:23.10-py3
```

如果不想使用docker，建议完整安装CUDA Toolkit，可以从官网下载并安装，安装完成后设置环境变量CUDA_HOME为CUDA Toolkit的安装路径，并确保编译nvcc器在你的文件夹中PATH，例如：

``` shell
export CUDA_HOME=/usr/local/cuda
export PATH="${CUDA_HOME}/bin:$PATH"
```

以下是一个健全性检查，以验证 CUDA 工具包是否正确安装：
``` shell
nvcc --version # verify that nvcc is in your PATH
${CUDA_HOME}/bin/nvcc --version # verify that nvcc is in your CUDA_HOME
```

### 不支持的操作系统
vLLM 只能在 Linux 上完全运行，但出于开发目的，您仍然可以在其他系统（例如 macOS）上构建它，以便导入和获得更方便的开发环境。二进制文件不会被编译，并且无法在非 Linux 系统上运行。

在安装之前只需禁用VLLM_TARGET_DEVICE环境变量：
``` shell
export VLLM_TARGET_DEVICE=empty
pip install -e .
```